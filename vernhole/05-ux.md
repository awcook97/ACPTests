Okay. I've read every file in this repo. Let me give you the full UX breakdown.

---

# UX Analysis: ACP Hub — Multi-Agent Orchestration TUI

## Who Is The User?

Let me start where every good UX analysis starts: **who is actually going to sit in front of this thing?**

The user is a developer — probably an experienced one — who:
1. Already uses AI coding agents (Codex, Copilot) individually
2. Wants multiple agents collaborating on the same task
3. Has heard of VernHole (the multi-persona discovery pattern) and wants that, but with real ACP agents instead of personality prompts
4. Is comfortable with terminals but does NOT want to babysit JSON-RPC traffic

The key insight here: **this user wants the power of multi-agent collaboration without needing to understand the plumbing.** They're not here to debug JSON-RPC. They're here to get work done faster.

---

## The User Journey (As It Stands Today)

Let me walk through what actually happens right now:

### Step 1: "How do I start?"
The user clones this repo and... sees a scaffold. The README says:
```
python3 main.py doctor
```

**Problem:** The user has to run `doctor` before they even know if this tool does anything useful for them. There's no "here's what this looks like when it's working" moment. No screenshot, no GIF, no demo. The first experience is a dependency check. That's like walking into a restaurant and the host asks you to verify the kitchen equipment.

### Step 2: Configuration
The user sees `acp-hub.example.json`. They need to:
- Know that Codex uses `codex app-server`
- Know that Copilot uses `copilot --acp --stdio`
- Know what "protocol" means in this context ("acp" vs "codex_app_server")
- Create a new `acp-hub.json` file

**Problem:** The config error message is actually decent — `"config file not found: acp-hub.json. Start from acp-hub.example.json and save as acp-hub.json."` — but the user is still expected to understand protocol identifiers. `"protocol": "codex_app_server"` vs `"protocol": "acp"` — what if I just want to add Claude? What protocol is that? The config doesn't tell me what's valid.

### Step 3: The TUI
When they run `uv run acp-hub tui --config acp-hub.example.json`, they get... three static panels that say TODO. The sub_title shows the journal path, which is nice for debugging but means nothing to a first-time user.

**The user sees:** Three empty boxes with placeholder text.
**The user expects:** Agents doing something.
**The user gets:** A layout mockup.

This is the **critical drop-off point**. Right now, 100% of users bounce here because there's literally nothing to interact with.

---

## Deep UX Issues (Prioritized)

### 1. THE EMPTY STATE PROBLEM (Critical)

Every single panel in the TUI will start empty. What does the user see when:
- No agents have emitted events yet? (Transcripts panel)
- No commands have been run? (Commands panel)
- No files have changed? (Files panel)

Right now: nothing. There's no loading state, no "waiting for agents to connect..." message, no progress indicator.

**What it should show:**
- Transcripts: "Connecting to codex... Connecting to copilot..." with a spinner
- Commands: "No commands executed yet. Commands will appear here as agents run tools."
- Files: "Watching: ./src, ./tests (0 changes detected)"

Empty states aren't edge cases. **They're the first thing every user sees.**

### 2. THE "WHAT'S HAPPENING?" PROBLEM (Critical)

The design doc says agents communicate via JSON-RPC over stdio. The user sees... raw JSON? Parsed text? The design mentions a toggle between "rendered" and "raw events" but doesn't define what "rendered" means.

When Agent A sends something to Agent B via the routing layer, what does the user see? A JSON blob? A formatted message? The current `Event` model has `kind` and `payload` — but there's no rendering strategy. The user will be drowning in `agent.jsonrpc` events with no way to understand the conversation at a glance.

**The user needs:** A conversation view. "Codex is working on X. Copilot suggested Y. Codex responded with Z." Not `{"jsonrpc":"2.0","method":"...","params":{...}}`.

### 3. THE "HOW DO I GIVE IT A TASK?" PROBLEM (Critical)

Here's the biggest UX gap in the entire system: **there is no way for the user to actually tell the agents what to do.**

The implementation plan has 10 tasks. None of them include "user input." The TUI has no input field. The CLI has no `--prompt` flag. The config file doesn't have a `task` field.

The VernHole pattern works because you say "analyze this idea" and multiple personas chew on it. This system spawns agents that... do what? Sit there? The user has no way to inject a task.

This is the #1 blocker. You are not the user — you understand the system. The user opens this, sees three panels, and asks: **"Cool, but how do I tell it what to do?"**

### 4. THE ROUTING UX PROBLEM (High)

Task 9 in the implementation plan defines three routing modes:
- Broadcast
- Round-robin
- "Moderator" mode (one agent coordinates others)

These are developer concepts, not user concepts. The user doesn't care about "routing topology." They care about:
- "I want both agents to work on this together" (collaboration)
- "I want to see what each agent thinks separately" (comparison)
- "I want one agent to lead and delegate" (orchestration)

**Rename these.** "Collaborate," "Compare," "Orchestrate." Or better yet — don't make the user choose. Start with the most useful default (probably orchestration) and let power users configure routing in the JSON file.

### 5. THE TRUST AND SAFETY UX (High)

The AGENTS.md says "Treat spawned agents as untrusted processes." Good principle. But the design says the hub is "the only component allowed to execute tools." This means the hub needs to ask the user for approval on dangerous operations.

The current design mentions "(Future) optional approvals: block risky tools behind a hub-side approval prompt." This can't be future. This is Day 1. If I spawn two AI agents and they both independently decide to `rm -rf` my project, and the hub just journals it and lets it happen... that's not a monitoring tool, that's a liability.

**The approval UX needs to be designed now:**
- What does the approval prompt look like in a TUI?
- Does it pause all agents or just the requesting one?
- What happens to the conversation flow during the pause?
- Can I set a "trust level" per agent?

### 6. THE COGNITIVE OVERLOAD PROBLEM (Medium)

Three simultaneous panels with real-time streaming data from N agents. The current three-column layout means each panel gets ~1/3 of terminal width. With 2 agents, the transcripts panel needs sub-tabs. With tool invocations streaming in the commands panel AND file changes in the files panel... the user's eyes are bouncing everywhere.

**Progressive disclosure matters here:**
- Default view: conversation summary (one pane, merged timeline)
- Expand: per-agent detail views
- Deep dive: raw events, JSON, diffs

Don't throw everything at the user simultaneously. That's a developer solution, not a user solution.

### 7. THE FIRST FIVE MINUTES (Medium)

There's no onboarding flow. No `acp-hub init` that asks:
- "Which agents do you have installed?"
- "What directory should I watch?"
- "Let me verify your agents work..."

Instead, the user is expected to hand-author JSON config, know the right CLI flags for each agent, and understand protocol identifiers. That's a recipe for a 95% drop-off in the first five minutes.

### 8. ERROR MESSAGES (Medium)

Let me call out some specific ones:

- `"missing required key: 'agents'"` — Good, tells you what's missing
- `"expected non-empty string for 'agents[0].id'"` — Good, path to the problem
- `"config must be a JSON object at the top level"` — Decent
- `"process not started or stdin unavailable"` — This is a developer error message. What does the user do with this?

The error handling in `proc.py` silently swallows journal errors (`except Exception: pass`). This means if the journal path is wrong, the user loses all their event data with zero indication. That's not a graceful failure, that's a silent data loss.

### 9. THE "IS IT DONE?" PROBLEM (Medium)

Multi-agent collaboration doesn't have a clear "done" state. When VernHole runs, each persona gives their take and it's visually clear when everyone has spoken. With real ACP agents... do they just run forever? How does the user know the task is complete?

There needs to be a concept of "session completion" — when all agents have stopped producing output and no tools are in flight, surface a clear "All agents idle. Task appears complete." message. Don't make the user stare at a quiet screen wondering if something's still happening.

### 10. THE JOURNAL ISN'T A USER FEATURE (Low... for now)

The design is very proud of the JSONL journal for replay/debugging. Cool. But from a UX perspective, `runs/latest/events.jsonl` is meaningless to the user. They don't want to grep JSONL. They want:
- "Show me what happened last time"
- "Replay that session"
- "What did Codex suggest for the auth refactor?"

The journal is an implementation detail masquerading as a feature. It needs a user-facing layer on top (search, replay UI, session naming).

---

## The Happy Path That Should Exist (But Doesn't Yet)

Here's what the first run SHOULD feel like:

```
$ acp-hub init
  Detecting installed agents...
  Found: codex (v4.2), copilot (v1.8)
  Workspace: /home/andrew/projects/my-app

  Created acp-hub.json with 2 agents.
  Run `acp-hub start` to begin.

$ acp-hub start "Refactor the auth module to use JWT"

  ┌─ ACP Hub ─────────────────────────────────────┐
  │                                                │
  │  Task: Refactor auth module to use JWT         │
  │  Agents: codex (working), copilot (thinking)   │
  │  Status: ● Active                              │
  │                                                │
  │  [codex] Analyzing current auth implementation │
  │  [codex] Found: session-based auth in 3 files  │
  │  [copilot] Reviewing JWT best practices...     │
  │  [codex] Proposing migration plan:             │
  │     1. Add jwt dependency                      │
  │     2. Create token utility...                 │
  │                                                │
  │  ⚠ codex wants to run: npm install jsonwebtoken│
  │  [Allow] [Deny] [Allow all from codex]         │
  │                                                │
  └────────────────────────────────────────────────┘
```

That's the experience. Conversation-first. Approvals inline. Status always visible. The user knows what's happening, who's doing it, and when they need to act.

---

## Summary: Top 5 Things To Fix Before Building More Backend

| Priority | Issue | Fix |
|----------|-------|-----|
| 1 | No way to input a task | Add `--task` / inline prompt / input field in TUI |
| 2 | No approval flow for tool execution | Design and implement the approval UX now, not "future" |
| 3 | Empty states everywhere | Design the loading, empty, and idle states for every panel |
| 4 | Raw JSON instead of conversation view | Build a rendered transcript view that summarizes agent activity in plain language |
| 5 | No onboarding | Add `acp-hub init` that auto-detects agents and generates config |

The backend architecture is honestly solid. The event model is clean, the journal is simple, the process management is thoughtful. But right now this is a tool built for the person who built it — not for the person who'll use it.

**You are not the user.** The user doesn't want to monitor JSON-RPC traffic. The user wants to say "do this thing" and watch smart agents figure it out together, stepping in only when asked. Design for that.

---

Why did the UX designer refuse to ship the multi-agent hub? Because the only user journey it supported was `git clone -> confusion -> close terminal`. The error message? "ERR_USER_NOT_FOUND: have you tried being the developer who wrote it?"

-- UX Vern *(you are not the user, but I am the user's lawyer)*
